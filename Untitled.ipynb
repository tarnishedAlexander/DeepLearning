{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc0945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esto solo es para una neural network de 2 capas profundas\n",
    "def initParameters(nx,nh,ny):\n",
    "    np.random.seed(1) #iniciar numeros randoms pero que cada vez que\n",
    "    #volvamos a ejecutar el programa siempre seran los mismos valores empezando desde 1\n",
    "    W1 = np.random.randn(nh,nx)*0.01\n",
    "    b1 = np.zeros((nh,1))\n",
    "    W2 = np.random.randn(ny,nh)*0.01\n",
    "    b2 = np.zeros((ny,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99826526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto es para una deep neural network\n",
    "def initParametersDeep(layerDims):\n",
    "    np.random.seed(4)\n",
    "    parameters={}\n",
    "    L = len(layerDims)\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = np.random.rand(layerDims[l], layerDims[l-1])*0.01\n",
    "        parameters[\"b\" + str(l)] = np.random.rand(layerDims[l], 1)\n",
    "        \n",
    "        assert parameters[\"W\" + str(l)].shape == (layerDims[l], layerDims[l-1])\n",
    "        assert parameters[\"b\"+ str(l)].shape == (layerDims[l], 1)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb8345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21c6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache =Z\n",
    "    assert (A.shape == Z.shape)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fa5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluBackward(dA,cache):\n",
    "    Z=cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b679c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidBackward(dA,cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500e5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearFordward(A,W,b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A,W,b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1487781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActFordward(APrev, W,b,act):\n",
    "    if act == \"sigmoid\":\n",
    "        Z, linearCache = linearFordward(APrev,W,b)\n",
    "        A, activationCache = sigmoid(Z)\n",
    "    if act == \"relu\":\n",
    "        Z, linearCache = linearFordward(APrev,W,b)\n",
    "        A, activationCache = relu(Z)\n",
    "    cache = (linearCache, activationCache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a228e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearModelFordward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        APrev = A\n",
    "        A, cache = linearActFordward(A, parameters['W' + str(L)],parameters['b' + str(L)] , \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linearActFordward(A, parameters['W' + str(L)],parameters['b' + str(L)] , \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(np.dot(Y, np.log(AL))+ np.dot((1-Y),np.log(1-AL)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9e613d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBackward(dZ, cache):\n",
    "    APrev, W,b = cache\n",
    "    m = APrev.shape[1]\n",
    "    dW = np.dot(dZ,APrev)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims= True)/m\n",
    "    dAPrev = np.dot(W.T, dZ)\n",
    "    return dAPrev, dW, dblinearFordward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c6b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearActBackward(dA, cache, activation):\n",
    "    linearCache, activationCache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = reluBackward(dA, activationCache)\n",
    "        dAPrev, dW, db = linearBackward(dZ, linearCache)\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoidBackward(dA, activationCache)\n",
    "        dAPrev, dW, db = linearBackward(dZ, linearCache)\n",
    "        \n",
    "    return dAPrev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8fd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LModelBackward(AL, L, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    currentCache = caches[L-1]\n",
    "    dAPrevTemp, dWTemp, dbTemp = linearActBackward(dAL, currentCache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dAPrevTemp\n",
    "    grads[\"dW\" + str(L)] = dWTemp\n",
    "    grads[\"db\" + str(L)] = dbTemp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20432336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(params, grads, learningRate):\n",
    "    parametes = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l)] -= learningRate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l)] -= learningRate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b42e8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predic(X,Y, parameters): \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters)\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas,caches = LinearModelFordward(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i]>0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95278e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLayerModel(X,Y, layerDims, learningRate = 0.0075, iter= 3000, printCost = False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initParametersDeep(layerDims)\n",
    "    for i in range(0, iter):\n",
    "        AL,caches = LinearModelFordward(X, parameters)\n",
    "        cost = computeCost(AL, Y)\n",
    "        grads = LModelBackward(AL, Y, caches)\n",
    "        parameters = updateParameters(parameters, grads learningRate)\n",
    "        \n",
    "        if printCost a and i % 100 == 0 or i==iter-1 : \n",
    "            print(\"the cost function {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or == iter:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96150f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
